{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2ogUuil6B6nh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "batch_size = 4\n",
        "data_text = \"hello world\"\n",
        "vocab = sorted(list(set(data_text)))\n",
        "vocab_size = len(vocab)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "epochs = 1000\n",
        "embed_size = 32\n"
      ],
      "metadata": {
        "id": "CVgW-_4eB_ix"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stoi = {ch: i for i, ch in enumerate(vocab)}\n",
        "itos = {i: ch for ch, i in stoi.items()}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])"
      ],
      "metadata": {
        "id": "GAEERtk-CSCw"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(encode(data_text), dtype=torch.long).to(device)"
      ],
      "metadata": {
        "id": "L9voHN-GLUSv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch():\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i + block_size] for i in ix])\n",
        "    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "J4R3I0F5La_J"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TinyLLM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.fc = nn.Linear(embed_size * block_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embed(x)               # (B, T, C)\n",
        "        x = x.view(x.size(0), -1)       # flatten: (B, T*C)\n",
        "        return self.fc(x)               # (B, vocab_size)\n"
      ],
      "metadata": {
        "id": "AZpeKXeTLdb2"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Model Setup ---\n",
        "model = TinyLLM().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "emHqV3BMLkuo"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- Training Loop ---\n",
        "for epoch in range(epochs):\n",
        "    x, y = get_batch()\n",
        "    logits = model(x)\n",
        "    loss = F.cross_entropy(logits, y[:, -1])  # Only last token predicted\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-jeiKiyLpvf",
        "outputId": "c2aeb1c9-0ea1-4ae1-9138-8134e38460b7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 2.4121\n",
            "Epoch 100, Loss: 0.0105\n",
            "Epoch 200, Loss: 0.0042\n",
            "Epoch 300, Loss: 0.0023\n",
            "Epoch 400, Loss: 0.0017\n",
            "Epoch 500, Loss: 0.0012\n",
            "Epoch 600, Loss: 0.0008\n",
            "Epoch 700, Loss: 0.0007\n",
            "Epoch 800, Loss: 0.0006\n",
            "Epoch 900, Loss: 0.0005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- Generation Function ---\n",
        "def generate(model, start='h', max_new_tokens=20):\n",
        "    model.eval()\n",
        "    idx = torch.tensor([encode(start)], dtype=torch.long).to(device)\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -block_size:]\n",
        "\n",
        "        # Padding if needed\n",
        "        if idx_cond.shape[1] < block_size:\n",
        "            padding = torch.zeros(1, block_size - idx_cond.shape[1], dtype=torch.long).to(device)\n",
        "            idx_cond = torch.cat([padding, idx_cond], dim=1)\n",
        "\n",
        "        logits = model(idx_cond)\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        next_id = torch.multinomial(probs, num_samples=1)\n",
        "        idx = torch.cat((idx, next_id), dim=1)\n",
        "\n",
        "    return decode(idx[0].tolist())\n",
        "\n",
        "# --- Generate text after training ---\n",
        "print(\"Generated text:\", generate(model))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f40zBxXvLvhV",
        "outputId": "7c5a36d4-fae1-4285-8bec-aff98fdb0a02"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text: hdrwdrldllrldrdrrddlr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EMHayKARL1p7"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}